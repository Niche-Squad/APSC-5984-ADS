{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APSC-5984 Lab 7: API and Database\n",
    "\n",
    "## 0. Overview\n",
    "\n",
    "API stands for Application Programming Interface. It is a set of tools meant to interact with other software or database. In this lab, we will use `requests` to interact with two USDA APIs. Later this week, we will also use `sqlite3` to practice building a database on our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. USDA local food portal API\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Before you can use the API, you need to get an API key and essentail parameters, such as the database you want to interact with and the query you want to run. Please treat the API key as a password and do not share it with others. Go to the [USDA local food portal](https://www.usdalocalfoodportal.com/fe/fregisterpublicapi) to register for an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = \"xxxxxxx\" # fill in your API key\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "}\n",
    "params = {\"apikey\": API, \"y\": 37.221508, \"x\": -80.423857, \"radius\": 100}\n",
    "base_url = \"https://www.usdalocalfoodportal.com/api/farmersmarket/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request responses\n",
    "\n",
    "The responses are usually coded as 3-digit numbers. The most common ones are:\n",
    "\n",
    "* 1xx - Informational response - the request was received, continuing process\n",
    "* 2xx - Success - the request was successfully received, understood, and accepted\n",
    "* 3xx - Redirection - further action needs to be taken in order to complete the request\n",
    "* 4xx - Client error - the request contains bad syntax or cannot be fulfilled\n",
    "* 5xx - Server error - the server failed to fulfill an apparently valid request\n",
    "\n",
    "In most cases, you want to get a 2xx response. If you get a 4xx or 5xx response, you need to check your code and make sure you are using the API correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(base_url, headers=headers, params=params)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing JSON\n",
    "\n",
    "The response from the API is usually in JSON format. JSON stands for JavaScript Object Notation. It is a lightweight data-interchange format. It is not easy for humans to read and write, hense we can call `.json()` and `pd.DataFrame()` to parse the response into a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = response.json()\n",
    "df = pd.DataFrame(json[\"data\"])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging the data\n",
    "\n",
    "The data might contain more information than you need. We can use Pandas techniques we learned before to rearrange the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"listing_name\",\n",
    "    \"brief_desc\",\n",
    "    \"contact_name\",\n",
    "    \"contact_phone\",\n",
    "    \"media_website\",\n",
    "    \"media_facebook\",\n",
    "    \"media_twitter\",\n",
    "    \"location_city\",\n",
    "    \"location_state\",\n",
    "    \"location_y\",\n",
    "    \"location_x\",\n",
    "]\n",
    "\n",
    "df.loc[:, cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agricultural Resource Management Survey (ARMS) API\n",
    "\n",
    "### API key\n",
    "\n",
    "Go to [ARMS API](https://www.ers.usda.gov/developer/data-apis/arms-data-api/#apiForm) to obtain an API key for the USDA ARMS API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = \"xxxxxxx\" # fill in your API key\n",
    "params = {\"api_key\": API}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `arms/year` to get the list of available years\n",
    "\n",
    "This API has a lot of data. Based on the documentation https://www.ers.usda.gov/developer/data-apis/arms-data-api/#apiForm, we can use `arms/year` to get the list of available years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arms/year\n",
    "url = \"https://api.ers.usda.gov/data/arms/year\"\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "json_data = response.json()\n",
    "json_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `arms/report` to get the list of available reports\n",
    "\n",
    "To make sure we are using the API correctly, we can use `arms/report` to get the list of available reports. We will later need to use the correct report name to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all report\n",
    "url = \"https://api.ers.usda.gov/data/arms/report\"\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "js = response.json()\n",
    "df = pd.DataFrame(response.json()[\"data\"])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Farm Business Income Statement\n",
    "\n",
    "Let's say we want to get the farm business income statement for all farms in the US for the years 2008 to 2022. We can start with a single year, say 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"api_key\": API,\n",
    "    \"Year\": 2018,\n",
    "    \"report\": \"Farm Business Income Statement\",\n",
    "}\n",
    "url = \"https://api.ers.usda.gov/data/arms/surveydata\"\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "json_data = response.json()\n",
    "data = pd.DataFrame(json_data[\"data\"])\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data\n",
    "\n",
    "You can check the data by inspecting the unique values of each column of interest. For example, if you want to know any category other than \"Operator Age\", you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"category\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to know what `Production Specialty` was recorded as. You can combine `df.query()` and `df.unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query(\"category == 'Production Specialty'\").loc[:, \"category_value\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check variables to know what attributes are available for that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query(\"category == 'Production Specialty'\").loc[:, \"variable_name\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "It is a good idea to wrap up the code into functions so that you can reuse them later. Especially when we deal with the queries with repeated patterns, we can use functions to make the code more concise.\n",
    "\n",
    "To design a function, first thing we can to define is the input and output. In our case, we provide API key and parameters (year, report name, etc.) as input and get a parsed dataframe as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(year, API, report=\"Farm Business Income Statement\"):\n",
    "    # inputs\n",
    "    params = {\n",
    "        \"api_key\": API,\n",
    "        \"Year\": year,\n",
    "        \"report\": report,\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    url = \"https://api.ers.usda.gov/data/arms/surveydata\"\n",
    "    # query\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    print(\"status code:\", response.status_code)\n",
    "    json_data = response.json()\n",
    "    data = pd.DataFrame(json_data[\"data\"])\n",
    "    # parse output\n",
    "    data = data.query(\"variable_name == 'Gross cash farm income'\")\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = query_data(2018, API)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define another function to process the data we get from the API. In this example, we will do the following processing:\n",
    "\n",
    "- Select the columns of interest\n",
    "- calculate standard deviation of the income statement\n",
    "- calculate 95% confidence interval of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(data):\n",
    "    cols = [\"year\", \"category\", \"category_value\", \"estimate\", \"median\", \"rse\"]\n",
    "    datasub = data.loc[:, cols]\n",
    "    datasub[\"se\"] = datasub[\"estimate\"] * (datasub[\"rse\"] / 100)\n",
    "    datasub[\"upper\"] = datasub[\"estimate\"] + datasub[\"se\"] * 1.96  # 95% CI\n",
    "    datasub[\"lower\"] = datasub[\"estimate\"] - datasub[\"se\"] * 1.96  # 95% CI\n",
    "    return datasub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check the results to make sure the function works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data18 = query_data(2018, API)\n",
    "data18 = post_process(data18)\n",
    "data18.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query multiple years\n",
    "\n",
    "With the functions we defined, it is now easier to query multiple years. We can use a for loop to iterate through the years and use the functions we defined to get the data. To avoid overloading the server, we can use `time.sleep()` to pause the code for one second between each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for year in range(2008, 2022):  # only 2008-2021\n",
    "    print(\"Loading data for year\", year)\n",
    "    data_query = query_data(year, API)\n",
    "    data_query = post_process(data_query)\n",
    "    data = pd.concat([data, data_query], axis=0)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Now, let's focus on the two categories: `NASS Region` and `Production Specialty`. We will use ggplot2 (`plotnine` library) to visualize the income statement for each category across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "dataplot = data.query(\"category in ['NASS Region', 'Production Specialty']\")\n",
    "dataplot[\"group\"] = dataplot[\"category\"] + \" - \" + dataplot[\"category_value\"]\n",
    "dataplot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(\n",
    "        dataplot.query(\"category == 'NASS Region'\"),\n",
    "        aes(x=\"year\", color=\"category_value\", group=\"group\"),\n",
    "    )\n",
    "    + geom_line(aes(y=\"estimate\", color=\"category_value\"))\n",
    "    + geom_line(aes(y=\"median\", color=\"category_value\"))\n",
    "    + geom_ribbon(aes(ymin=\"lower\", ymax=\"upper\", fill=\"category_value\"), alpha=0.2)\n",
    "    + theme(figure_size=(10, 10))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By production specialty. We only show the livestock categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(\n",
    "        dataplot.query(\n",
    "            \"category == 'Production Specialty' and category_value in ['Dairy', 'Cattle', 'Hogs', 'Poultry']\"\n",
    "        ),\n",
    "        aes(\n",
    "            x=\"year\",\n",
    "            color=\"category_value\",\n",
    "            fill=\"category_value\",\n",
    "            group=\"category_value\",\n",
    "        ),\n",
    "    )\n",
    "    + geom_line(aes(y=\"estimate\"))\n",
    "    + geom_line(aes(y=\"median\"))\n",
    "    + geom_ribbon(aes(ymin=\"lower\", ymax=\"upper\"), alpha=0.2)\n",
    "    # color theme\n",
    "    + scale_color_brewer(type=\"qual\", palette=\"Set2\")\n",
    "    + scale_fill_brewer(type=\"qual\", palette=\"Set2\")\n",
    "    + theme(figure_size=(10, 10))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a log transformation to the data to make the plot more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataplot[\"log_estimate\"] = np.log(dataplot[\"estimate\"])\n",
    "dataplot[\"log_upper\"] = np.log(dataplot[\"upper\"])\n",
    "dataplot[\"log_lower\"] = np.log(dataplot[\"lower\"])\n",
    "dataplot[\"log_median\"] = np.log(dataplot[\"median\"])\n",
    "(\n",
    "    ggplot(\n",
    "        dataplot.query(\n",
    "            \"category == 'Production Specialty' and category_value in ['Dairy', 'Cattle', 'Hogs', 'Poultry']\"\n",
    "        ),\n",
    "        aes(\n",
    "            x=\"year\",\n",
    "            color=\"category_value\",\n",
    "            fill=\"category_value\",\n",
    "            group=\"category_value\",\n",
    "        ),\n",
    "    )\n",
    "    + geom_line(aes(y=\"log_estimate\"), size=1)\n",
    "    + geom_line(aes(y=\"log_median\"))\n",
    "    + geom_ribbon(aes(ymin=\"log_lower\", ymax=\"log_upper\"), alpha=0.2)\n",
    "    + scale_color_brewer(type=\"qual\", palette=\"Set2\")\n",
    "    + scale_fill_brewer(type=\"qual\", palette=\"Set2\")\n",
    "    + theme(figure_size=(10, 10))\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQLite3\n",
    "\n",
    "After we learn how to interact with an existing database through API, we can also build our own database using our own data. In this section, we will use `sqlite3` to build a database and query the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a database\n",
    "\n",
    "Creating a database is as simple as creating a file. We can use `sqlite3.connect()` to create a database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"demo.db\") # conn stands for connection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we learned in the previous section, a database can contain multiple tables (or surveys, reports). To craete a table, we need to specify the name of the columns and the data type. We also need to specify the primary key, which is a unique identifier for each row. We can create a table named `users` with the following columns:\n",
    "\n",
    "- `id` - INTEGER (PRIMARY KEY)\n",
    "- `name` - TEXT\n",
    "- `gender` - TEXT\n",
    "- `age` - INTEGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor() # cur stands for cursor\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        gender TEXT,\n",
    "        age INTEGER\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"PRAGMA table_info(users)\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think the command is too complicated, we can use a function to simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cols(cur, table):\n",
    "    cur.execute(\"PRAGMA table_info(%s)\" % table)\n",
    "    return display([x[1:3] for x in cur.fetchall()])\n",
    "\n",
    "list_cols(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the database\n",
    "\n",
    "We can use `conn.commit()` to save the changes to the database. Like how we deal with a file, we need to close `conn.close()` the database after we are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use a `with` statement to automatically close the database after we are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"demo.db\") as conn:\n",
    "    cur = conn.cursor()\n",
    "    list_cols(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data\n",
    "\n",
    "There are two ways to insert data into a table:\n",
    "\n",
    "Provide information for all columns\n",
    "- `INSERT INTO users VALUES (1, 'John', 'M', 20)`\n",
    "\n",
    "Provide information for some columns\n",
    "- `INSERT INTO users (name, gender) VALUES ('John', 'M')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"INSERT INTO users VALUES (1, 'Mary', 'F', 25)\")\n",
    "cur.execute(\"INSERT INTO users (name) VALUES ('John')\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM users\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or wrap it up in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(cur, table):\n",
    "    cur.execute(\"SELECT * FROM %s\" % table)\n",
    "    display(cur.fetchall())\n",
    "\n",
    "print_table(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually use `df.to_sql()` to insert data into a table. Parameters we need to consider:\n",
    "\n",
    "- `if_exists`: If the table already exists, we can choose to `replace` the table, or `append` the data to the existing table.\n",
    "- `index`: whether to include the index of the dataframe as a column in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"Camille\", \"Mike\", \"Jason\", \"Maria\"],\n",
    "        \"gender\": [\"female\", \"male\", \"male\", \"female\"],\n",
    "        \"age\": [40, 25, 35, 20],\n",
    "    }\n",
    ")\n",
    "df.to_sql(\"users\", conn, if_exists=\"append\", index=False)  # if_exists=\"replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add constraints to columns\n",
    "\n",
    "You might notice that the gender values were not in a consistent format, which should either be [`M`, `F`] or [`Male`, `Female`]. We can use `CHECK` to add constraints to the columns.\n",
    "\n",
    "Before re-creating the table, we need to drop the table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DROP TABLE users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a new table with the constraints:\n",
    "\n",
    "* id - INTEGER (PRIMARY KEY)\n",
    "* name - TEXT - NOT NULL\n",
    "* gender - TEXT - can only be either 'male' or 'female'\n",
    "* age - INTEGER - must be in the range of 0 to 150\n",
    "* weight - REAL - must be in the range of 0 to 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        gender TEXT CHECK(gender IN (\"male\", \"female\")),\n",
    "        age INTEGER CHECK(age >= 0 AND age <= 150),\n",
    "        weight REAL CHECK(weight >= 0 AND weight <= 300)\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to insert data again. We can start with expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"INSERT INTO users VALUES (1, 'Mary', 'female', 25, 150)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"INSERT INTO users VALUES (2, 'Mary', 'f', 25, 150)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"INSERT INTO users VALUES (2, 'Mary', 'female', -3, 200)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting constraints to the columns, it is easier to ensure the data quality when the database is growing. Here is a complete code for creating a `user` table with data inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"demo.db\") as conn:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        gender TEXT CHECK(gender IN (\"male\", \"female\")),\n",
    "        age INTEGER CHECK(age >= 0 AND age <= 150),\n",
    "        weight REAL CHECK(weight >= 0 AND weight <= 300)\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": [\"Camille\", \"Mike\", \"Jason\", \"Maria\"],\n",
    "            \"gender\": [\"female\", \"male\", \"male\", \"female\"],\n",
    "            \"age\": [40, 25, 35, 20],\n",
    "            \"weight\": [150, 200, 180, 120],\n",
    "        }\n",
    "    )\n",
    "    df.to_sql(\"users\", conn, if_exists=\"append\", index=False)\n",
    "    print_table(cur, \"users\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference integrity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a table `users` to define users' information. Now, let's create another table `walks` to record the walking activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"demo.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE walks (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        user_id INTEGER NOT NULL,\n",
    "        date TEXT NOT NULL,\n",
    "        distance FLOAT NOT NULL,\n",
    "        duration INTEGER NOT NULL,\n",
    "        FOREIGN KEY (user_id) REFERENCES users (id))\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we set `user_id` as the foreign key. This means that the value of `user_id` must correspond to the value of `id` in the `users` table when we need to consider the relationship between the two tables. This is called a `reference integrity`. We can use `REFERENCES` to set the reference integrity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `walks` table include the information of the walking date, distance, and duration.Let's insert data into this `walks` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": [1, 1, 1, 2, 2, 3, 3, 4, 4, 4],\n",
    "        \"date\": [\n",
    "            \"02-26-2023\",\n",
    "            \"02-27-2023\",\n",
    "            \"02-28-2023\",\n",
    "            \"02-26-2023\",\n",
    "            \"02-27-2023\",\n",
    "            \"02-26-2023\",\n",
    "            \"02-27-2023\",\n",
    "            \"02-26-2023\",\n",
    "            \"02-27-2023\",\n",
    "            \"02-28-2023\",\n",
    "        ],\n",
    "        \"distance\": [1.2, 1.5, 1.7, 2.2, 2.5, 3.2, 3.5, 4.2, 4.5, 4.7],\n",
    "        \"duration\": [30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "    }\n",
    ")\n",
    "data.to_sql(\"walks\", conn, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(cur, \"walks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have two tables with data inserted. We can use `JOIN` to put the data from two tables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT * FROM walks JOIN users\n",
    "    ON walks.user_id = users.id\n",
    "    \"\"\"\n",
    ")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the user information was added to the `walks` table. This is similar to how we use `df.merge()` to combine two dataframes in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_sql(\"SELECT * FROM users\", conn)\n",
    "df_walks = pd.read_sql(\"SELECT * FROM walks\", conn)\n",
    "pd.merge(df_walks, df_users, left_on=\"user_id\", right_on=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQlite3 VS. Pandas\n",
    "\n",
    "Here we will put the major functionalities of SQLite3 and Pandas side by side to see how they compare.\n",
    "\n",
    "#### Sorting\n",
    "\n",
    "SQLite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM users ORDER BY age DESC\")  # or ASC\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.sort_values(by=\"age\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "\n",
    "SQLite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT age, gender FROM users WHERE age > 30\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.loc[:, [\"age\", \"gender\"]].query(\"age > 30\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT name, weight FROM users\n",
    "    WHERE name LIKE '%m%'\n",
    "    OR name LIKE '%n%'\n",
    "    \"\"\"\n",
    ")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.loc[:, [\"name\", \"weight\"]].query(\n",
    "    \"\"\"\n",
    "    name.str.upper().str.contains('M') |\\\n",
    "    name.str.upper().str.contains('N')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping\n",
    "\n",
    "SQLite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT user_id, avg(distance), sum(duration), count(distance)\n",
    "    FROM walks GROUP BY user_id\n",
    "    \"\"\"\n",
    ")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_walks.groupby(\"user_id\").aggregate(\n",
    "    distance_mean=(\"distance\", \"mean\"),\n",
    "    duration_sum=(\"duration\", \"sum\"),\n",
    "    distance_count=(\"distance\", \"count\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sqlite_master\n",
    "\n",
    "`sqliet_master` is a system table that contains the information of all tables in the database. We can use `SELECT * FROM sqlite_master` to get the information of all tables. The output will look like this:\n",
    "\n",
    "* `type`: the type of the object. In this case, it is `table`.\n",
    "* `name`: the name of the table.\n",
    "* `tbl_name`: the name of the table.\n",
    "* `rootpage`: the page number of the root b-tree page for the table.\n",
    "* `sql`: the SQL statement used to create the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM sqlite_master\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply list all the tables in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cols(cur, table):\n",
    "    cur.execute(\"PRAGMA table_info(%s)\" % table)\n",
    "    cols = [x[1:3] for x in cur.fetchall()]\n",
    "    return display(cols)\n",
    "\n",
    "def print_table(cur, table):\n",
    "    cur.execute(\"SELECT * FROM %s\" % table)\n",
    "    output = cur.fetchall()\n",
    "    display(output)\n",
    "\n",
    "def clean_table(cur, table):\n",
    "    cur.execute(\"DELETE FROM %s\" % table)\n",
    "\n",
    "def drop_table(cur, table):\n",
    "    cur.execute(\"DROP TABLE %s\" % table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niche",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad532947ed7ac808c0a842d8ff1aa280c18aa45f795e2b1f2d64f29153af8175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
